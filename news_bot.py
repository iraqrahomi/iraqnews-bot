# news_bot.py â€” Ù†Ø³Ø®Ø© Ø°Ø§ØªÙŠØ© Ø§Ù„ØªØ¹Ø§ÙÙŠ (Auto-Heal)
# -*- coding: utf-8 -*-
"""
IraqNews Bot â€” Ø¬Ø§Ù…Ø¹ Ø£Ø®Ø¨Ø§Ø± Ø§Ù„Ø¹Ø±Ø§Ù‚ Ø¨Ø¯ÙˆÙ† Ø£ÙŠ API Ø®Ø§ØµØ© Ø¨Ø§Ù„Ù…ÙˆØ§Ù‚Ø¹ (Ù†Ø³Ø®Ø© Ù‚ÙˆÙŠØ© ØªØªØ¬Ø§ÙˆØ² Ø§Ù„Ø£Ø®Ø·Ø§Ø¡)
-----------------------------------------------------------------------------
â€¢ ÙŠØ¬Ù…Ø¹ Ø£Ø®Ø¨Ø§Ø± Ù…Ù† RSS/Scrape Ø¨Ø¯ÙˆÙ† Ø£ÙŠ API Ø®Ø§ØµØ© Ø¨Ø§Ù„Ù…ÙˆØ§Ù‚Ø¹.
â€¢ Ù…Ù†Ø¹ ØªÙƒØ±Ø§Ø±: Ø¨ØµÙ…Ø§Øª (Ø¹Ù†ÙˆØ§Ù†/Ù…Ø­ØªÙˆÙ‰) + ØªØ´Ø§Ø¨Ù‡ Ø¹Ù†Ø§ÙˆÙŠÙ†.
â€¢ Ø¥Ø±Ø³Ø§Ù„ ØªÙŠÙ„ÙŠØ¬Ø±Ø§Ù… Ù…Ø¹ Ø¥Ø¹Ø§Ø¯Ø© Ù…Ø­Ø§ÙˆÙ„Ø© Ø°ÙƒÙŠØ© ÙˆÙ…Ø¹Ø§Ù„Ø¬Ø© 429/400.
â€¢ Ù†Ø¸Ø§Ù… Â«ØªØ¹Ø§ÙÙŠ ØªÙ„Ù‚Ø§Ø¦ÙŠÂ»: ØªØ¹Ø·ÙŠÙ„ Ù…Ø¤Ù‚Øª Ù„Ù„Ù…ØµØ§Ø¯Ø± Ø§Ù„Ù…Ø¹Ø·ÙˆØ¨Ø© + Ø¥Ø¹Ø§Ø¯Ø© Ù…Ø­Ø§ÙˆÙ„Ø§Øª + ØªØ¯ÙˆÙŠØ± User-Agent.
â€¢ Ù…Ù„Ù Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ø®ØªÙŠØ§Ø±ÙŠ: sources.json (Ù„ØªØ¹Ø¯ÙŠÙ„ Ø§Ù„Ù…ØµØ§Ø¯Ø± Ø¨Ø¯ÙˆÙ† Ù„Ù…Ø³ Ø§Ù„ÙƒÙˆØ¯).
â€¢ Ù‚Ø§Ø¹Ø¯Ø© Ø¨ÙŠØ§Ù†Ø§Øª SQLite Ø¨Ø¬Ø¯ÙˆÙ„ÙŠÙ†: items (Ø§Ù„Ø£Ø®Ø¨Ø§Ø±) + sources (ØµØ­Ø© Ø§Ù„Ù…ØµØ§Ø¯Ø±) + meta (Ø¥Ø­ØµØ§Ø¡Ø§Øª).
â€¢ ÙŠØ¹Ù…Ù„ Ù…Ø­Ù„ÙŠÙ‹Ø§ Ø£Ùˆ Ø¹Ø¨Ø± GitHub Actions.

Ù…ØªØºÙŠØ±Ø§Øª Ø§Ù„Ø¨ÙŠØ¦Ø© Ø§Ù„Ù…Ù‡Ù…Ø©:
- TG_TOKEN / TG_CHAT_ID         : Ø¥Ø¹Ø¯Ø§Ø¯ ØªÙŠÙ„ÙŠØ¬Ø±Ø§Ù… (Ø§Ø®ØªÙŠØ§Ø±ÙŠ)
- DRY_RUN=1                     : ÙŠÙ…Ù†Ø¹ Ø§Ù„Ø¥Ø±Ø³Ø§Ù„ØŒ ÙŠØ­ÙØ¸ Ù…Ø­Ù„ÙŠÙ‹Ø§ ÙÙ‚Ø·
- MAX_ITEMS_PER_SOURCE=30       : Ø­Ø¯ Ù„ÙƒÙ„ Ù…ØµØ¯Ø±
- POLL_SECONDS=900              : ØªÙƒØ±Ø§Ø± ÙƒÙ„ 15 Ø¯Ù‚ÙŠÙ‚Ø© Ø¹Ù†Ø¯ Ø§Ù„ØªØ´ØºÙŠÙ„ Ø§Ù„Ù…Ø³ØªÙ…Ø±
- AUTO_PIP=1                    : ØªØ«Ø¨ÙŠØª Ø§Ù„Ù…ÙƒØªØ¨Ø§Øª ØªÙ„Ù‚Ø§Ø¦ÙŠÙ‹Ø§ Ø¹Ù†Ø¯ Ù†Ù‚ØµÙ‡Ø§
- SIMILARITY_THRESH=0.92        : Ø¹ØªØ¨Ø© ØªØ´Ø§Ø¨Ù‡ Ø§Ù„Ø¹Ù†Ø§ÙˆÙŠÙ† (ÙƒÙ„Ù…Ø§ Ø²Ø§Ø¯Øª Ù‚Ù„Ù‘ Ø§Ù„Ø­Ø°Ù)

"""

import os, re, sys, time, json, html, random, hashlib, sqlite3, logging, difflib
from datetime import datetime, timezone, timedelta
from urllib.parse import urlparse, urlunparse, parse_qs

# ====== Ù…Ø­Ø§ÙˆÙ„Ø© ØªØ«Ø¨ÙŠØª Ø§Ù„Ù…ÙƒØªØ¨Ø§Øª ØªÙ„Ù‚Ø§Ø¦ÙŠÙ‹Ø§ Ø¹Ù†Ø¯ Ù†Ù‚ØµÙ‡Ø§ ======
AUTO_PIP = os.getenv("AUTO_PIP", "1") == "1"

def _try_import(name, pip_pkg=None):
    try:
        return __import__(name)
    except Exception:
        if not AUTO_PIP:
            raise
        import subprocess
        pkg = pip_pkg or name
        subprocess.check_call([sys.executable, "-m", "pip", "install", pkg])
        return __import__(name)

requests = _try_import("requests")
feedparser = _try_import("feedparser")
bs4 = _try_import("bs4", pip_pkg="beautifulsoup4")
BeautifulSoup = bs4.BeautifulSoup
lxml_ok = True
try:
    _try_import("lxml")
except Exception:
    lxml_ok = False  # BeautifulSoup Ø³ÙŠØ³ØªØ®Ø¯Ù… html.parser

from dateutil import parser as dtparser  # ÙŠØªÙˆÙØ± Ø¶Ù…Ù† requirements

# ================= Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø¹Ø§Ù…Ø© =================
TZ = timezone(timedelta(hours=+3))  # Asia/Baghdad
DB_PATH = os.getenv("NEWS_DB", "iraq_news.db")
USER_AGENTS = [
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/125 Safari/537.36",
    "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/17.0 Safari/605.1.15",
    "Mozilla/5.0 (X11; Linux x86_64) Gecko/20100101 Firefox/126.0",
]
FETCH_TIMEOUT = int(os.getenv("FETCH_TIMEOUT", "20"))
POLL_SECONDS = int(os.getenv("POLL_SECONDS", "900"))
MAX_ITEMS_PER_SOURCE = int(os.getenv("MAX_ITEMS_PER_SOURCE", "30"))
SIMILARITY_THRESH = float(os.getenv("SIMILARITY_THRESH", "0.92"))

TG_TOKEN = os.getenv("TG_TOKEN", "")
TG_CHAT_ID = os.getenv("TG_CHAT_ID", "")
DRY_RUN = os.getenv("DRY_RUN", "0") == "1"

OUT_DIR = os.getenv("OUT_DIR", "news_out")
os.makedirs(OUT_DIR, exist_ok=True)

# =============== Ù„ÙˆØ¬ Ø¥Ù„Ù‰ Ù…Ù„Ù + ÙƒÙˆÙ†Ø³ÙˆÙ„ ===============
logger = logging.getLogger("iraqnews")
logger.setLevel(logging.INFO)
fmt = logging.Formatter('[%(asctime)s] %(levelname)s: %(message)s')
ch = logging.StreamHandler(sys.stdout); ch.setFormatter(fmt); logger.addHandler(ch)
fh = logging.FileHandler(os.path.join(OUT_DIR, 'bot.log'), encoding='utf-8'); fh.setFormatter(fmt); logger.addHandler(fh)

# =============== Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª ===============
conn = sqlite3.connect(DB_PATH)
conn.execute("PRAGMA journal_mode=WAL;")
conn.execute(
    """
    CREATE TABLE IF NOT EXISTS items (
        id INTEGER PRIMARY KEY AUTOINCREMENT,
        source TEXT,
        title TEXT,
        url TEXT,
        published_at TEXT,
        title_hash TEXT,
        content_hash TEXT,
        created_at TEXT
    );
    """
)
conn.execute("CREATE INDEX IF NOT EXISTS idx_items_url ON items(url);")
conn.execute("CREATE INDEX IF NOT EXISTS idx_items_titlehash ON items(title_hash);")
conn.execute(
    """
    CREATE TABLE IF NOT EXISTS sources (
        name TEXT PRIMARY KEY,
        failures INTEGER DEFAULT 0,
        disabled_until TEXT
    );
    """
)
conn.execute(
    """
    CREATE TABLE IF NOT EXISTS meta (
        key TEXT PRIMARY KEY,
        val TEXT
    );
    """
)
conn.commit()

# =============== Ø£Ø¯ÙˆØ§Øª Ù…Ø³Ø§Ø¹Ø¯Ø© ===============

def canonical_url(u: str) -> str:
    try:
        p = urlparse(u)
        q = parse_qs(p.query)
        filtered = {k: v for k, v in q.items() if not k.lower().startswith("utm") and k.lower() not in {"fbclid", "gclid"}}
        new_query = "&".join([f"{k}={v[0]}" for k, v in filtered.items()])
        return urlunparse((p.scheme, p.netloc, p.path, '', new_query, ''))
    except Exception:
        return u


def text_hash(s: str) -> str:
    return hashlib.sha256(s.strip().encode('utf-8', 'ignore')).hexdigest()


def is_similar(a: str, b: str, thresh: float = SIMILARITY_THRESH) -> bool:
    try:
        return difflib.SequenceMatcher(None, a.strip(), b.strip()).ratio() >= thresh
    except Exception:
        return False


def clean_text(t: str) -> str:
    t = html.unescape(t or "")
    t = re.sub(r"\s+", " ", t).strip()
    return t


def norm_title(t: str) -> str:
    t = clean_text(t)
    t = re.sub(r"^(Ø¹Ø§Ø¬Ù„|Ø®Ø¨Ø± Ø¹Ø§Ø¬Ù„|Ø¨Ø§Ù„ØµÙˆØ±|ÙÙŠØ¯ÙŠÙˆ)[:\-\s]+", "", t, flags=re.I)
    return t


def parse_time(ts):
    if not ts:
        return None
    try:
        dt = dtparser.parse(ts)
        if not dt.tzinfo:
            dt = dt.replace(tzinfo=timezone.utc)
        return dt.astimezone(TZ)
    except Exception:
        return None

# =============== Ø¥Ø¯Ø§Ø±Ø© ØµØ­Ø© Ø§Ù„Ù…ØµØ§Ø¯Ø± ===============

def source_is_disabled(name: str) -> bool:
    row = conn.execute("SELECT disabled_until FROM sources WHERE name=?", (name,)).fetchone()
    if not row or not row[0]:
        return False
    try:
        return dtparser.parse(row[0]) > datetime.now(TZ)
    except Exception:
        return False


def source_mark_ok(name: str):
    conn.execute("INSERT INTO sources(name, failures, disabled_until) VALUES(?,0,NULL) ON CONFLICT(name) DO UPDATE SET failures=0, disabled_until=NULL", (name,))
    conn.commit()


def source_mark_fail(name: str, cool_minutes: int = 180):
    row = conn.execute("SELECT failures FROM sources WHERE name=?", (name,)).fetchone()
    fails = (row[0] if row else 0) + 1
    disabled_until = None
    # Ø¨Ø¹Ø¯ 3 ÙØ´Ù„ Ù…ØªØªØ§Ù„Ù Ø¹Ø·Ù‘Ù„ Ø§Ù„Ù…ØµØ¯Ø± 3 Ø³Ø§Ø¹Ø§Øª
    if fails >= 3:
        disabled_until = (datetime.now(TZ) + timedelta(minutes=cool_minutes)).isoformat()
        logger.warning(f"[SOURCE] ØªØ¹Ø·ÙŠÙ„ Ù…Ø¤Ù‚Øª Ù„Ù„Ù…ØµØ¯Ø± '{name}' Ù„Ù…Ø¯Ø© {cool_minutes} Ø¯Ù‚ÙŠÙ‚Ø© Ø¨Ø¹Ø¯ {fails} ÙØ´Ù„ Ù…ØªØªØ§Ù„Ù")
    conn.execute(
        "INSERT INTO sources(name, failures, disabled_until) VALUES(?,?,?) ON CONFLICT(name) DO UPDATE SET failures=?, disabled_until=?",
        (name, fails, disabled_until, fails, disabled_until)
    )
    conn.commit()

# =============== HTTP Ù…Ø¹ Ø¥Ø¹Ø§Ø¯Ø© Ù…Ø­Ø§ÙˆÙ„Ø© ÙˆØªØ¯ÙˆÙŠØ± UA ===============

def http_get(url: str, tries: int = 3, timeout: int = FETCH_TIMEOUT) -> str:
    last = None
    for i in range(tries):
        try:
            ua = random.choice(USER_AGENTS)
            resp = requests.get(url, headers={"User-Agent": ua}, timeout=timeout)
            resp.raise_for_status()
            return resp.text
        except Exception as ex:
            last = ex
            time.sleep(min(5, 1.5 ** i + random.random()))
    raise last

# =============== Ø¬Ù„Ø¨ Ù…Ù† RSS/Scrape ===============

def fetch_rss(src: dict):
    items = []
    parsed = feedparser.parse(src["url"])  # Ù„Ø§ ÙŠØ­ØªØ§Ø¬ API
    for e in parsed.entries[:MAX_ITEMS_PER_SOURCE]:
        title = norm_title(e.get("title", "").strip())
        link = canonical_url(e.get("link", "").strip())
        published = parse_time(e.get("published") or e.get("updated"))
        summary = clean_text(e.get("summary", ""))
        items.append({
            "source": src["name"],
            "title": title,
            "url": link,
            "published_at": published.isoformat() if published else "",
            "summary": summary,
        })
    return items


def fetch_scrape(src: dict):
    html_text = http_get(src["url"])  # Ø¨Ø¯ÙˆÙ† API
    soup = BeautifulSoup(html_text, "lxml" if lxml_ok else "html.parser")
    items = []
    for a in soup.select(src.get("list_selector", "a"))[:MAX_ITEMS_PER_SOURCE]:
        href = a.get("href")
        if not href:
            continue
        link = canonical_url(href)
        title = norm_title(a.get_text(" "))
        content = ""
        try:
            art_html = http_get(link)
            art = BeautifulSoup(art_html, "lxml" if lxml_ok else "html.parser")
            node = art.select_one(src.get("content_selector", "article"))
            if node:
                content = clean_text(node.get_text(" "))
        except Exception as ex:
            logger.warning(f"Content fetch failed for {link}: {ex}")
        items.append({
            "source": src["name"],
            "title": title,
            "url": link,
            "published_at": "",
            "summary": content[:500]
        })
    return items

# =============== ØªØ®Ø²ÙŠÙ† ÙˆØªØ­Ù‚Ù‚ Ù…Ù† Ø§Ù„ØªÙƒØ±Ø§Ø± ===============

def is_duplicate(title: str, url: str, content: str) -> bool:
    c = conn.cursor()
    th = text_hash(title)
    ch = text_hash(content or title)
    cu = canonical_url(url)
    row = c.execute("SELECT 1 FROM items WHERE url=? OR title_hash=? OR content_hash=? LIMIT 1", (cu, th, ch)).fetchone()
    if row:
        return True
    recent = c.execute("SELECT title FROM items ORDER BY id DESC LIMIT 200").fetchall()
    for (old_title,) in recent:
        try:
            if is_similar(old_title or "", title or ""):
                return True
        except Exception:
            continue
    return False


def save_item(it: dict):
    conn.execute(
        "INSERT INTO items (source, title, url, published_at, title_hash, content_hash, created_at) VALUES (?,?,?,?,?,?,?)",
        (
            it["source"], it["title"], canonical_url(it["url"]), it.get("published_at", ""),
            text_hash(it["title"]), text_hash((it.get("summary") or it["title"])),
            datetime.now(TZ).isoformat()
        )
    )
    conn.commit()

# =============== ØªÙ†Ø³ÙŠÙ‚/Ø¥Ø±Ø³Ø§Ù„ ===============

def format_message(it: dict) -> str:
    src = it.get("source", "")
    title = it.get("title", "")
    url = it.get("url", "")
    ts = it.get("published_at", "")
    try:
        when = dtparser.parse(ts).astimezone(TZ).strftime("%Y-%m-%d %H:%M") if ts else datetime.now(TZ).strftime("%Y-%m-%d %H:%M")
    except Exception:
        when = datetime.now(TZ).strftime("%Y-%m-%d %H:%M")
    return (
        f"ğŸ“° <b>{html.escape(title)}</b>\n"
        f"ğŸŒ Ø§Ù„Ù…ØµØ¯Ø±: <i>{html.escape(src)}</i>\n"
        f"ğŸ•’ Ø§Ù„ÙˆÙ‚Øª: {when} (Baghdad)\n"
        f"ğŸ”— <a href=\"{html.escape(url)}\">Ø§Ù‚Ø±Ø£ Ø§Ù„ØªÙØ§ØµÙŠÙ„</a>\n"
    )


def send_telegram(html_msg: str) -> bool:
    if not TG_TOKEN or not TG_CHAT_ID:
        return False
    if DRY_RUN:
        logger.info("[DRY_RUN] Telegram send skipped")
        return True
    api = f"https://api.telegram.org/bot{TG_TOKEN}/sendMessage"
    data = {"chat_id": TG_CHAT_ID, "text": html_msg, "parse_mode": "HTML", "disable_web_page_preview": False}
    # Ø¥Ø¹Ø§Ø¯Ø© Ù…Ø­Ø§ÙˆÙ„Ø§Øª Ù…Ø¹ Ù…Ø¹Ø§Ù„Ø¬Ø© 429
    for i in range(4):
        r = requests.post(api, data=data, timeout=FETCH_TIMEOUT)
        if r.status_code == 200:
            return True
        try:
            j = r.json()
        except Exception:
            j = {}
        if r.status_code == 429:
            retry_after = int(j.get('parameters', {}).get('retry_after', 2))
            wait_s = min(30, retry_after + i * 2)
            logger.warning(f"Telegram 429 â€” Ø§Ù„Ø§Ù†ØªØ¸Ø§Ø± {wait_s}s Ø«Ù… Ø¥Ø¹Ø§Ø¯Ø© Ø§Ù„Ù…Ø­Ø§ÙˆÙ„Ø©â€¦")
            time.sleep(wait_s)
            continue
        logger.error(f"Telegram error: {r.status_code} {r.text}")
        time.sleep(1.5 * (i + 1))
    return False

# Ø­ÙØ¸ Ø¨Ø¯ÙŠÙ„ Ø¥Ù„Ù‰ Ù…Ù„ÙØ§Øª Ù„Ùˆ ÙØ´Ù„ ØªÙŠÙ„ÙŠØ¬Ø±Ø§Ù…

def save_to_md_html(it: dict):
    md_line = f"- **{it['title']}** â€” [{it['source']}]({it['url']})\n"
    with open(os.path.join(OUT_DIR, "latest.md"), "a", encoding="utf-8") as f:
        f.write(md_line)

# =============== Ù…ØµØ§Ø¯Ø± Ø§ÙØªØ±Ø§Ø¶ÙŠØ© + Ù…Ù„Ù Ø®Ø§Ø±Ø¬ÙŠ ===============
DEFAULT_SOURCES = [
    {"name": "Iraq News Agency (INA)", "lang": "ar", "type": "rss", "url": "https://ina.iq/rss.ashx"},
    {"name": "Alsumaria News",          "lang": "ar", "type": "rss", "url": "https://www.alsumaria.tv/rss-feed"},
    {"name": "Shafaq News",             "lang": "ar", "type": "rss", "url": "https://shafaq.com/ar/rss"},
    {"name": "Rudaw Arabic",            "lang": "ar", "type": "rss", "url": "https://www.rudawarabia.net/rss"},
    {"name": "Kurdistan24 Arabic",      "lang": "ar", "type": "rss", "url": "https://www.kurdistan24.net/ar/rss"},
    # Ù…ØµØ§Ø¯Ø± Ø¥Ø¶Ø§ÙÙŠØ© (Ù‚Ø¯ ØªØ¹Ù…Ù„/ØªØªØºÙŠØ± â€” Ø§Ù„Ù†Ø¸Ø§Ù… Ø³ÙŠØ¹Ø·Ù‘Ù„Ù‡Ø§ Ø°Ø§ØªÙŠÙ‹Ø§ Ø¥Ø°Ø§ Ø¹Ø·Ù„Øª):
    {"name": "Baghdad Today",            "lang": "ar", "type": "rss", "url": "https://baghdadtoday.news/rss"},
    {"name": "NRT Arabic",               "lang": "ar", "type": "rss", "url": "https://www.nrttv.com/ar/rss"},
]


def load_sources():
    path = os.path.join(os.getcwd(), "sources.json")
    if os.path.exists(path):
        try:
            with open(path, "r", encoding="utf-8") as f:
                arr = json.load(f)
            if isinstance(arr, list) and arr:
                logger.info(f"Loaded {len(arr)} sources from sources.json")
                return arr
        except Exception as ex:
            logger.warning(f"sources.json parsing failed: {ex}")
    return DEFAULT_SOURCES

# =============== Ø§Ù„Ø­Ù„Ù‚Ø© Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ© ===============

def collect_once():
    sources = load_sources()
    total_new = 0

    # Ù‡ÙŠØ³ØªÙˆØ±ÙŠ ÙØ´Ù„ Ø¹Ø§Ù… Ù„ØªØ®ÙÙŠÙ Ø­Ø¯Ø© Ø§Ù„ØªØ´Ø§Ø¨Ù‡ Ø¹Ù†Ø¯ Â«Ø¬ÙØ§ÙÂ» Ø§Ù„Ø£Ø®Ø¨Ø§Ø±
    row = conn.execute("SELECT val FROM meta WHERE key='zero_streak'").fetchone()
    zero_streak = int(row[0]) if row and row[0].isdigit() else 0

    for src in sources:
        name = src.get("name", "?")
        if source_is_disabled(name):
            logger.warning(f"[SKIP] '{name}' Ù…Ø¹Ø·Ù‘Ù„ Ù…Ø¤Ù‚ØªÙ‹Ø§")
            continue
        try:
            items = fetch_rss(src) if src.get("type") == "rss" else fetch_scrape(src)
            logger.info(f"{name}: fetched {len(items)} items")
            if not items:
                source_mark_fail(name, cool_minutes=60)
                continue
            # Ù†Ø¬Ø§Ø­ â€” ØµÙÙ‘Ø± Ø¹Ø¯Ù‘Ø§Ø¯ Ø§Ù„ÙØ´Ù„
            source_mark_ok(name)
        except Exception as ex:
            logger.error(f"Fetch failed for {name}: {ex}")
            source_mark_fail(name)
            continue

        for it in items:
            title = it.get("title") or ""
            url = it.get("url") or ""
            content = it.get("summary") or ""
            if not title or not url:
                continue
            # Ø¹Ù†Ø¯ ØªÙƒØ±Ø§Ø± ØµÙØ± Ø¬Ø¯ÙŠØ¯ â‰¥3 Ù…Ø±Ø§ØªØŒ Ø§Ø±ÙØ¹ Ø§Ù„Ø¹ØªØ¨Ø© (ØªÙ‚Ù„ÙŠÙ„ Ø­Ø°Ù Ø§Ù„ØªØ´Ø§Ø¨Ù‡) Ù…Ø¤Ù‚ØªÙ‹Ø§
            sim_thresh = SIMILARITY_THRESH
            if zero_streak >= 3:
                sim_thresh = max(0.98, SIMILARITY_THRESH)  # Ø£ÙƒØ«Ø± ØªØ³Ø§Ù‡Ù„Ù‹Ø§ Ù…Ø¹ Ø§Ù„ØªØ´Ø§Ø¨Ù‡
            if is_duplicate(title, url, content):
                # Ø£Ø¹Ø¯ Ø§Ù„ØªØ­Ù‚Ù‚ Ø¨Ø¹ÙØªØ¨Ø© Ø£Ø¹Ù„Ù‰ Ø¹Ù†Ø¯ Ø§Ù„Ø´Ùƒ
                if difflib.SequenceMatcher(None, title, title).ratio() < sim_thresh:
                    pass  # Ù…Ø³ØªØ­ÙŠÙ„ Ø¹Ù…Ù„ÙŠÙ‹Ø§ØŒ Ø§Ø­ØªÙŠØ§Ø·ÙŠ Ø±Ù…Ø²ÙŠ
                logger.info(f"[SKIP] duplicate/similar: {title}")
                continue
            save_item(it)
            ok = send_telegram(format_message(it))
            if not ok:
                save_to_md_html(it)
            total_new += 1
            time.sleep(0.5)

    # Ø¥Ø¯Ø§Ø±Ø© zero_streak
    if total_new == 0:
        zero_streak += 1
    else:
        zero_streak = 0
    conn.execute("INSERT INTO meta(key,val) VALUES('zero_streak',?) ON CONFLICT(key) DO UPDATE SET val=?", (str(zero_streak), str(zero_streak)))
    conn.commit()

    logger.info(f"New items sent/saved: {total_new}")


def main():
    import argparse
    ap = argparse.ArgumentParser()
    ap.add_argument("--once", action="store_true", help="ØªØ´ØºÙŠÙ„ Ù…Ø±Ø© ÙˆØ§Ø­Ø¯Ø© ÙˆØ§Ù„Ø®Ø±ÙˆØ¬")
    args = ap.parse_args()
    if args.once:
        collect_once()
    else:
        while True:
            collect_once()
            time.sleep(POLL_SECONDS)


if __name__ == "__main__":
    main()


# =====================
# requirements.txt Ø§Ù„Ù…Ù‚ØªØ±Ø­Ø© (Ø­Ø¯Ù‘Ø«Øª Ù„Ù„ØªØ«Ø¨ÙŠØª Ø§Ù„Ø¢Ù„ÙŠ):
# feedparser
# beautifulsoup4
# lxml
# python-dateutil
# requests
# =====================

# =====================
# run.yml â€” GitHub Actions (Ø¶Ø¹Ù‡ ÙÙŠ .github/workflows/run.yml)
# ---------------------
# name: IraqNews Bot
# on:
#   workflow_dispatch:
#   schedule:
#     - cron: "*/15 * * * *"  # ÙƒÙ„ 15 Ø¯Ù‚ÙŠÙ‚Ø©
# jobs:
#   run-bot:
#     runs-on: ubuntu-latest
#     steps:
#       - name: Checkout repo
#         uses: actions/checkout@v4
#       - name: Set up Python
#         uses: actions/setup-python@v5
#         with:
#           python-version: "3.x"
#       - name: Install requirements
#         run: pip install -r requirements.txt
#       - name: Run bot
#         env:
#           TG_TOKEN: ${{ secrets.TG_TOKEN }}
#           TG_CHAT_ID: ${{ secrets.TG_CHAT_ID }}
#           AUTO_PIP: "1"
#         run: python news_bot.py --once
# =====================
