# news_bot.py â€” Autoâ€‘Heal + Facebook Composer + Image Grabber
# -*- coding: utf-8 -*-
"""
IraqNews Bot â€” Ø¬Ø§Ù…Ø¹ Ø£Ø®Ø¨Ø§Ø± Ø§Ù„Ø¹Ø±Ø§Ù‚ Ø¨Ø¯ÙˆÙ† Ø£ÙŠ API Ø®Ø§ØµØ© Ø¨Ø§Ù„Ù…ÙˆØ§Ù‚Ø¹ (Ù†Ø³Ø®Ø© Ø§Ø­ØªØ±Ø§ÙÙŠØ© Ù†Ù‡Ø§Ø¦ÙŠØ©)
------------------------------------------------------------------------------
â€¢ ÙŠØ¬Ù…Ø¹ Ø£Ø®Ø¨Ø§Ø± Ù…Ù† RSS/Scrape Ø¨Ø¯ÙˆÙ† Ø£ÙŠ API.
â€¢ Ù…Ù†Ø¹ Ø§Ù„ØªÙƒØ±Ø§Ø± (Ø¨ØµÙ…Ø§Øª + ØªØ´Ø§Ø¨Ù‡ Ø¹Ù†Ø§ÙˆÙŠÙ†) + Ù‚Ø§Ø¹Ø¯Ø© Ø¨ÙŠØ§Ù†Ø§Øª SQLite.
â€¢ Ø¥Ø±Ø³Ø§Ù„ ØªÙŠÙ„ÙŠØ¬Ø±Ø§Ù… Ù…Ø¹ Ø¥Ø¹Ø§Ø¯Ø© Ù…Ø­Ø§ÙˆÙ„Ø§Øª Ø°ÙƒÙŠØ© (429/Ø£Ø®Ø·Ø§Ø¡ Ù…Ø¤Ù‚ØªØ©).
â€¢ ØªØ¹Ø§ÙÙ ØªÙ„Ù‚Ø§Ø¦ÙŠ Ù„Ù„Ù…ØµØ§Ø¯Ø±: ØªØ¹Ø·ÙŠÙ„ Ù…Ø¤Ù‚Øª Ø¨Ø¹Ø¯ ÙƒØ«Ø±Ø© Ø§Ù„ÙØ´Ù„ + ØªØ¯ÙˆÙŠØ± User-Agent.
â€¢ Ù…ÙØ±ÙƒÙ‘Ø¨ Ù…Ù†Ø´ÙˆØ±Ø§Øª ÙÙŠØ³Ø¨ÙˆÙƒ (Facebook Composer) Ø¬Ø§Ù‡Ø² Ù„Ù„Ù†Ø´Ø± + ØªÙ†Ø²ÙŠÙ„ ØµÙˆØ± og:image ØªÙ„Ù‚Ø§Ø¦ÙŠØ§Ù‹.
â€¢ Ù…Ù„ÙØ§Øª Ø¬Ø§Ù‡Ø²Ø©: Ù†Øµ Ø§Ù„Ù…Ù†Ø´ÙˆØ± + Ø§Ù„ØµÙˆØ± ÙÙŠ Ù…Ø¬Ù„Ø¯Ø§Øª Ù…Ù†Ø¸Ù…Ø© Ø­Ø³Ø¨ Ø§Ù„ØªØ§Ø±ÙŠØ®.
â€¢ Ù…Ù„Ù Ù…ØµØ§Ø¯Ø± Ø®Ø§Ø±Ø¬ÙŠ Ø§Ø®ØªÙŠØ§Ø±ÙŠ: sources.json.

ENV:
- TG_TOKEN / TG_CHAT_ID             : Ù„Ø¥Ø±Ø³Ø§Ù„ ØªÙŠÙ„ÙŠØ¬Ø±Ø§Ù… (Ø§Ø®ØªÙŠØ§Ø±ÙŠ)
- DRY_RUN=1                         : ÙŠÙ…Ù†Ø¹ Ø§Ù„Ø¥Ø±Ø³Ø§Ù„ ÙˆÙŠÙƒØªØ¨ Ù…Ù„ÙØ§Øª ÙÙ‚Ø·
- MAX_ITEMS_PER_SOURCE=30           : Ø­Ø¯ Ø¬Ù„Ø¨ Ù„ÙƒÙ„ Ù…ØµØ¯Ø±
- POLL_SECONDS=900                  : ÙØªØ±Ø© Ø§Ù„ØªÙƒØ±Ø§Ø± Ø¹Ù†Ø¯ Ø§Ù„ØªØ´ØºÙŠÙ„ Ø§Ù„Ù…Ø³ØªÙ…Ø±
- AUTO_PIP=1                        : ØªØ«Ø¨ÙŠØª ØªÙ„Ù‚Ø§Ø¦ÙŠ Ù„Ù„Ø­Ø²Ù… Ø§Ù„Ù†Ø§Ù‚ØµØ©
- SIMILARITY_THRESH=0.92            : Ø¹ØªØ¨Ø© ØªØ´Ø§Ø¨Ù‡ Ø§Ù„Ø¹Ù†Ø§ÙˆÙŠÙ†
- FACEBOOK_MODE=1                   : ØªÙØ¹ÙŠÙ„ ØªÙˆÙ„ÙŠØ¯ Ù…Ù†Ø´ÙˆØ±Ø§Øª ÙÙŠØ³Ø¨ÙˆÙƒ ÙˆØ­ÙØ¸Ù‡Ø§
- FACEBOOK_TEMPLATE=short|summary|qa|bilingual : Ø§Ø®ØªÙŠØ§Ø± Ø§Ù„Ù‚Ø§Ù„Ø¨
- FACEBOOK_MAX_IMAGES=3             : Ø£Ù‚ØµÙ‰ Ø¹Ø¯Ø¯ ØµÙˆØ± ØªÙØ±ÙÙ‚ Ù„Ù„Ù…Ù†Ø´ÙˆØ±

Ø§Ù„Ù…Ø®Ø±Ø¬Ø§Øª:
- news_out/bot.log                  : Ø³Ø¬Ù„ Ø§Ù„ØªÙ†ÙÙŠØ°
- news_out/latest.md                : Ø³Ø¬Ù„ Ù…Ø®ØªØµØ± Ù„Ù„Ø£Ø®Ø¨Ø§Ø±
- news_out/facebook/YYYY-MM-DD/slug.txt         : Ù†Øµ Ù…Ù†Ø´ÙˆØ± ÙÙŠØ³Ø¨ÙˆÙƒ Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠ
- news_out/images/YYYY-MM-DD/slug_1.jpg ...     : ØµÙˆØ± Ù…Ø±Ø§ÙÙ‚Ø© Ø¥Ù† ØªÙˆÙØ±Øª
"""

import os, re, sys, time, json, html, random, hashlib, sqlite3, logging, difflib
from datetime import datetime, timezone, timedelta
from urllib.parse import urlparse, urlunparse, parse_qs, urljoin

# ====== ØªØ«Ø¨ÙŠØª ØªÙ„Ù‚Ø§Ø¦ÙŠ Ù„Ù„Ø­Ø²Ù… Ø¹Ù†Ø¯ Ù†Ù‚ØµÙ‡Ø§ ======
AUTO_PIP = os.getenv("AUTO_PIP", "1") == "1"

def _try_import(name, pip_pkg=None):
    try:
        return __import__(name)
    except Exception:
        if not AUTO_PIP:
            raise
        import subprocess
        pkg = pip_pkg or name
        subprocess.check_call([sys.executable, "-m", "pip", "install", pkg])
        return __import__(name)

requests   = _try_import("requests")
feedparser = _try_import("feedparser")
bs4        = _try_import("bs4", pip_pkg="beautifulsoup4")
BeautifulSoup = bs4.BeautifulSoup
try:
    _try_import("lxml")
    PARSER = "lxml"
except Exception:
    PARSER = "html.parser"

dtparser = _try_import("dateutil.parser", pip_pkg="python-dateutil").parser

# ================= Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø¹Ø§Ù…Ø© =================
TZ = timezone(timedelta(hours=+3))  # Asia/Baghdad
DB_PATH = os.getenv("NEWS_DB", "iraq_news.db")
USER_AGENTS = [
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/125 Safari/537.36",
    "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/17.0 Safari/605.1.15",
    "Mozilla/5.0 (X11; Linux x86_64) Gecko/20100101 Firefox/126.0",
]
FETCH_TIMEOUT = int(os.getenv("FETCH_TIMEOUT", "20"))
POLL_SECONDS = int(os.getenv("POLL_SECONDS", "900"))
MAX_ITEMS_PER_SOURCE = int(os.getenv("MAX_ITEMS_PER_SOURCE", "30"))
SIMILARITY_THRESH = float(os.getenv("SIMILARITY_THRESH", "0.92"))

TG_TOKEN = os.getenv("TG_TOKEN", "")
TG_CHAT_ID = os.getenv("TG_CHAT_ID", "")
DRY_RUN = os.getenv("DRY_RUN", "0") == "1"

FACEBOOK_MODE = os.getenv("FACEBOOK_MODE", "0") == "1"
FACEBOOK_TEMPLATE = os.getenv("FACEBOOK_TEMPLATE", "short").lower()
FACEBOOK_MAX_IMAGES = int(os.getenv("FACEBOOK_MAX_IMAGES", "3"))

OUT_DIR = os.getenv("OUT_DIR", "news_out")
os.makedirs(OUT_DIR, exist_ok=True)

# =============== Ù„ÙˆØ¬ Ø¥Ù„Ù‰ Ù…Ù„Ù + ÙƒÙˆÙ†Ø³ÙˆÙ„ ===============
logger = logging.getLogger("iraqnews")
logger.setLevel(logging.INFO)
fmt = logging.Formatter('[%(asctime)s] %(levelname)s: %(message)s')
ch = logging.StreamHandler(sys.stdout); ch.setFormatter(fmt); logger.addHandler(ch)
fh = logging.FileHandler(os.path.join(OUT_DIR, 'bot.log'), encoding='utf-8'); fh.setFormatter(fmt); logger.addHandler(fh)

# =============== Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª ===============
conn = sqlite3.connect(DB_PATH)
conn.execute("PRAGMA journal_mode=WAL;")
conn.execute(
    """
    CREATE TABLE IF NOT EXISTS items (
        id INTEGER PRIMARY KEY AUTOINCREMENT,
        source TEXT,
        title TEXT,
        url TEXT,
        published_at TEXT,
        title_hash TEXT,
        content_hash TEXT,
        created_at TEXT
    );
    """
)
conn.execute("CREATE INDEX IF NOT EXISTS idx_items_url ON items(url);")
conn.execute("CREATE INDEX IF NOT EXISTS idx_items_titlehash ON items(title_hash);")
conn.execute(
    """
    CREATE TABLE IF NOT EXISTS sources (
        name TEXT PRIMARY KEY,
        failures INTEGER DEFAULT 0,
        disabled_until TEXT
    );
    """
)
conn.execute(
    """
    CREATE TABLE IF NOT EXISTS meta (
        key TEXT PRIMARY KEY,
        val TEXT
    );
    """
)
conn.commit()

# =============== Ø£Ø¯ÙˆØ§Øª Ù…Ø³Ø§Ø¹Ø¯Ø© ===============

def ensure_dir(p: str):
    os.makedirs(p, exist_ok=True)


def canonical_url(u: str) -> str:
    try:
        p = urlparse(u)
        q = parse_qs(p.query)
        filtered = {k: v for k, v in q.items() if not k.lower().startswith("utm") and k.lower() not in {"fbclid", "gclid"}}
        new_query = "&".join([f"{k}={v[0]}" for k, v in filtered.items()])
        return urlunparse((p.scheme, p.netloc, p.path, '', new_query, ''))
    except Exception:
        return u


def text_hash(s: str) -> str:
    return hashlib.sha256(s.strip().encode('utf-8', 'ignore')).hexdigest()


def is_similar(a: str, b: str, thresh: float = SIMILARITY_THRESH) -> bool:
    try:
        return difflib.SequenceMatcher(None, a.strip(), b.strip()).ratio() >= thresh
    except Exception:
        return False


def clean_text(t: str) -> str:
    t = html.unescape(t or "")
    t = re.sub(r"\s+", " ", t).strip()
    return t


def norm_title(t: str) -> str:
    t = clean_text(t)
    t = re.sub(r"^(Ø¹Ø§Ø¬Ù„|Ø®Ø¨Ø± Ø¹Ø§Ø¬Ù„|Ø¨Ø§Ù„ØµÙˆØ±|ÙÙŠØ¯ÙŠÙˆ)[:\-\s]+", "", t, flags=re.I)
    return t


def parse_time(ts):
    if not ts:
        return None
    try:
        dt = dtparser.parse(ts)
        if not dt.tzinfo:
            dt = dt.replace(tzinfo=timezone.utc)
        return dt.astimezone(TZ)
    except Exception:
        return None


def slugify(s: str, maxlen: int = 60) -> str:
    s = re.sub(r"[\W_]+", "-", s.strip(), flags=re.UNICODE)
    s = re.sub(r"-+", "-", s).strip("-")
    return (s[:maxlen]).strip("-") or "post"

# =============== Ø¥Ø¯Ø§Ø±Ø© ØµØ­Ø© Ø§Ù„Ù…ØµØ§Ø¯Ø± ===============

def source_is_disabled(name: str) -> bool:
    row = conn.execute("SELECT disabled_until FROM sources WHERE name=?", (name,)).fetchone()
    if not row or not row[0]:
        return False
    try:
        return dtparser.parse(row[0]) > datetime.now(TZ)
    except Exception:
        return False


def source_mark_ok(name: str):
    conn.execute("INSERT INTO sources(name, failures, disabled_until) VALUES(?,0,NULL) ON CONFLICT(name) DO UPDATE SET failures=0, disabled_until=NULL", (name,))
    conn.commit()


def source_mark_fail(name: str, cool_minutes: int = 180):
    row = conn.execute("SELECT failures FROM sources WHERE name=?", (name,)).fetchone()
    fails = (row[0] if row else 0) + 1
    disabled_until = None
    if fails >= 3:
        disabled_until = (datetime.now(TZ) + timedelta(minutes=cool_minutes)).isoformat()
        logger.warning(f"[SOURCE] ØªØ¹Ø·ÙŠÙ„ Ù…Ø¤Ù‚Øª Ù„Ù„Ù…ØµØ¯Ø± '{name}' Ù„Ù…Ø¯Ø© {cool_minutes} Ø¯Ù‚ÙŠÙ‚Ø© Ø¨Ø¹Ø¯ {fails} ÙØ´Ù„")
    conn.execute(
        "INSERT INTO sources(name, failures, disabled_until) VALUES(?,?,?) ON CONFLICT(name) DO UPDATE SET failures=?, disabled_until=?",
        (name, fails, disabled_until, fails, disabled_until)
    )
    conn.commit()

# =============== HTTP Ù…Ø¹ Ø¥Ø¹Ø§Ø¯Ø© Ø§Ù„Ù…Ø­Ø§ÙˆÙ„Ø© ===============

def http_get(url: str, tries: int = 3, timeout: int = FETCH_TIMEOUT) -> str:
    last = None
    for i in range(tries):
        try:
            ua = random.choice(USER_AGENTS)
            resp = requests.get(url, headers={"User-Agent": ua}, timeout=timeout)
            resp.raise_for_status()
            return resp.text
        except Exception as ex:
            last = ex
            time.sleep(min(5, 1.5 ** i + random.random()))
    raise last

# =============== Ø¬Ù„Ø¨ Ù…Ù† RSS/Scrape ===============

def fetch_rss(src: dict):
    items = []
    parsed = feedparser.parse(src["url"])  # Ù„Ø§ ÙŠØ­ØªØ§Ø¬ API
    for e in parsed.entries[:MAX_ITEMS_PER_SOURCE]:
        title = norm_title(e.get("title", "").strip())
        link = canonical_url(e.get("link", "").strip())
        published = parse_time(e.get("published") or e.get("updated"))
        summary = clean_text(e.get("summary", ""))
        items.append({
            "source": src["name"],
            "title": title,
            "url": link,
            "published_at": published.isoformat() if published else "",
            "summary": summary,
        })
    return items


def fetch_scrape(src: dict):
    html_text = http_get(src["url"])  # Ø¨Ø¯ÙˆÙ† API
    soup = BeautifulSoup(html_text, PARSER)
    items = []
    for a in soup.select(src.get("list_selector", "a"))[:MAX_ITEMS_PER_SOURCE]:
        href = a.get("href")
        if not href:
            continue
        link = canonical_url(urljoin(src["url"], href))
        title = norm_title(a.get_text(" "))
        content = ""
        try:
            art_html = http_get(link)
            art = BeautifulSoup(art_html, PARSER)
            node = art.select_one(src.get("content_selector", "article"))
            if node:
                content = clean_text(node.get_text(" "))
        except Exception as ex:
            logger.warning(f"Content fetch failed for {link}: {ex}")
        items.append({
            "source": src["name"],
            "title": title,
            "url": link,
            "published_at": "",
            "summary": content[:500]
        })
    return items

# =============== ØªÙƒØ±Ø§Ø± ÙˆØªØ®Ø²ÙŠÙ† ===============

def is_duplicate(title: str, url: str, content: str) -> bool:
    c = conn.cursor()
    th = text_hash(title)
    ch = text_hash(content or title)
    cu = canonical_url(url)
    if c.execute("SELECT 1 FROM items WHERE url=? OR title_hash=? OR content_hash=? LIMIT 1", (cu, th, ch)).fetchone():
        return True
    for (old_title,) in c.execute("SELECT title FROM items ORDER BY id DESC LIMIT 200").fetchall():
        try:
            if is_similar(old_title or "", title or ""):
                return True
        except Exception:
            continue
    return False


def save_item(it: dict):
    conn.execute(
        "INSERT INTO items (source, title, url, published_at, title_hash, content_hash, created_at) VALUES (?,?,?,?,?,?,?)",
        (
            it["source"], it["title"], canonical_url(it["url"]), it.get("published_at", ""),
            text_hash(it["title"]), text_hash((it.get("summary") or it["title"])),
            datetime.now(TZ).isoformat()
        )
    )
    conn.commit()

# =============== Telegram & Files ===============

def format_message(it: dict) -> str:
    src = it.get("source", "")
    title = it.get("title", "")
    url = it.get("url", "")
    ts = it.get("published_at", "")
    try:
        when = dtparser.parse(ts).astimezone(TZ).strftime("%Y-%m-%d %H:%M") if ts else datetime.now(TZ).strftime("%Y-%m-%d %H:%M")
    except Exception:
        when = datetime.now(TZ).strftime("%Y-%m-%d %H:%M")
    return (
        f"ğŸ“° <b>{html.escape(title)}</b>\n"
        f"ğŸŒ Ø§Ù„Ù…ØµØ¯Ø±: <i>{html.escape(src)}</i>\n"
        f"ğŸ•’ Ø§Ù„ÙˆÙ‚Øª: {when} (Baghdad)\n"
        f"ğŸ”— <a href=\"{html.escape(url)}\">Ø§Ù‚Ø±Ø£ Ø§Ù„ØªÙØ§ØµÙŠÙ„</a>\n"
    )


def send_telegram(html_msg: str) -> bool:
    if not TG_TOKEN or not TG_CHAT_ID:
        return False
    if DRY_RUN:
        logger.info("[DRY_RUN] Telegram send skipped")
        return True
    api = f"https://api.telegram.org/bot{TG_TOKEN}/sendMessage"
    data = {"chat_id": TG_CHAT_ID, "text": html_msg, "parse_mode": "HTML", "disable_web_page_preview": False}
    for i in range(4):
        r = requests.post(api, data=data, timeout=FETCH_TIMEOUT)
        if r.status_code == 200:
            return True
        try:
            j = r.json()
        except Exception:
            j = {}
        if r.status_code == 429:
            retry_after = int(j.get('parameters', {}).get('retry_after', 2))
            wait_s = min(30, retry_after + i * 2)
            logger.warning(f"Telegram 429 â€” Ø§Ù„Ø§Ù†ØªØ¸Ø§Ø± {wait_s}s Ø«Ù… Ø¥Ø¹Ø§Ø¯Ø© Ø§Ù„Ù…Ø­Ø§ÙˆÙ„Ø©â€¦")
            time.sleep(wait_s)
            continue
        logger.error(f"Telegram error: {r.status_code} {r.text}")
        time.sleep(1.5 * (i + 1))
    return False


def save_to_md(it: dict):
    md_line = f"- **{it['title']}** â€” [{it['source']}]({it['url']})\n"
    with open(os.path.join(OUT_DIR, "latest.md"), "a", encoding="utf-8") as f:
        f.write(md_line)

# =============== Facebook Composer + Image Grabber ===============

def extract_og_images(page_url: str):
    """ÙŠØ±Ø¬Ø¹ Ù‚Ø§Ø¦Ù…Ø© ØµÙˆØ± Ù…Ø­ØªÙ…Ù„Ø© Ù…Ù† og:image ÙˆØ¨Ø¯Ø§Ø¦Ù„ Ø¨Ø³ÙŠØ·Ø©."""
    imgs = []
    try:
        html_text = http_get(page_url, tries=3)
        soup = BeautifulSoup(html_text, PARSER)
        # og:image
        for tag in soup.select('meta[property="og:image"], meta[name="og:image"]'):
            u = tag.get("content")
            if u:
                imgs.append(urljoin(page_url, u.strip()))
        # twitter:image
        for tag in soup.select('meta[name="twitter:image"], meta[property="twitter:image"]'):
            u = tag.get("content")
            if u:
                imgs.append(urljoin(page_url, u.strip()))
        # Ø£ÙˆÙ„ ØµÙˆØ±Ø© Ø¶Ù…Ù† Ø§Ù„Ù…Ù‚Ø§Ù„ ÙƒØ®ÙŠØ§Ø± Ø£Ø®ÙŠØ±
        art = soup.select_one("article") or soup
        for im in art.select("img"):
            u = im.get("src") or im.get("data-src")
            if u and not u.startswith("data:"):
                imgs.append(urljoin(page_url, u.strip()))
    except Exception as ex:
        logger.warning(f"extract_og_images failed: {ex}")
    # ØªÙ†Ø¸ÙŠÙ ÙˆØªÙˆØ­ÙŠØ¯
    uniq, seen = [], set()
    for u in imgs:
        if u and u not in seen:
            seen.add(u); uniq.append(u)
    return uniq


def download_images(urls, base_dir, base_name, max_n=FACEBOOK_MAX_IMAGES):
    ensure_dir(base_dir)
    saved = []
    for i, u in enumerate(urls[:max_n], start=1):
        try:
            ua = random.choice(USER_AGENTS)
            r = requests.get(u, headers={"User-Agent": ua}, timeout=FETCH_TIMEOUT)
            r.raise_for_status()
            ext = ".jpg"
            ct = r.headers.get("Content-Type", "").lower()
            if "png" in ct: ext = ".png"
            elif "jpeg" in ct or "jpg" in ct: ext = ".jpg"
            elif "webp" in ct: ext = ".webp"
            out_path = os.path.join(base_dir, f"{base_name}_{i}{ext}")
            with open(out_path, "wb") as f:
                f.write(r.content)
            saved.append(out_path)
        except Exception as ex:
            logger.warning(f"download image failed for {u}: {ex}")
    return saved


def compose_fb_text(it: dict, template: str = FACEBOOK_TEMPLATE) -> str:
    title = it.get("title", "").strip()
    src   = it.get("source", "").strip()
    url   = it.get("url", "").strip()
    ts    = it.get("published_at")
    when  = (parse_time(ts) or datetime.now(TZ)).strftime("%Y-%m-%d %H:%M")
    summary = (it.get("summary") or "").strip()
    short_sum = summary[:200] + ("â€¦" if len(summary) > 200 else "")

    if template == "short":
        return (f"ğŸ“° {title}\n"
                f"Ø§Ù„Ù…ØµØ¯Ø±: {src} | Ø§Ù„ØªÙˆÙ‚ÙŠØª: {when} (Ø¨ØºØ¯Ø§Ø¯)\n"
                f"ğŸ”— {url}\n"
                f"#Ø£Ø®Ø¨Ø§Ø±_Ø§Ù„Ø¹Ø±Ø§Ù‚ #Ø¨ØºØ¯Ø§Ø¯ #Ø¹Ø§Ø¬Ù„")
    if template == "summary":
        return (f"ğŸ“° {title}\n{short_sum}\n\n"
                f"ğŸŒ Ø§Ù„Ù…ØµØ¯Ø±: {src}\nğŸ•’ {when} (Ø¨ØºØ¯Ø§Ø¯)\n"
                f"ğŸ”— {url}\n#Ø£Ø®Ø¨Ø§Ø±_Ø§Ù„Ø¹Ø±Ø§Ù‚ #Ø§Ù„Ø¹Ø±Ø§Ù‚")
    if template == "qa":
        return (f"ğŸ—£ï¸ {title}\nØ¨Ø±Ø£ÙŠÙƒÙ…: Ø´Ù†Ùˆ ØªØ£Ø«ÙŠØ± Ù‡Ø§Ù„Ø®Ø¨Ø± Ù…Ø­Ù„ÙŠÙ‹Ø§ØŸ\n\n"
                f"Ø§Ù„Ù…ØµØ¯Ø±: {src} | {when} (Ø¨ØºØ¯Ø§Ø¯)\n"
                f"ğŸ”— {url}\n#Ø£Ø®Ø¨Ø§Ø±_Ø§Ù„Ø¹Ø±Ø§Ù‚ #Ù†Ù‚Ø§Ø´")
    if template == "bilingual":
        en = short_sum[:180]
        return (f"ğŸ“° {title}\n{short_sum}\n\n[EN] {en}\n\n"
                f"Source: {src} | Baghdad Time: {when}\n"
                f"ğŸ”— {url}\n#IraqNews #Baghdad")
    # default
    return (f"ğŸ“° {title}\n{short_sum}\n\n"
            f"ğŸŒ Ø§Ù„Ù…ØµØ¯Ø±: {src}\nğŸ•’ {when} (Ø¨ØºØ¯Ø§Ø¯)\n"
            f"ğŸ”— {url}")


def handle_facebook(it: dict):
    """ÙŠÙ†Ø´Ø¦ Ù…Ù„Ù Ù…Ù†Ø´ÙˆØ± ÙÙŠØ³Ø¨ÙˆÙƒ + ØªÙ†Ø²ÙŠÙ„ ØµÙˆØ± ÙˆÙŠØ±Ø¬Ù‘Ø¹ Ø§Ù„Ù…Ø³Ø§Ø±Ø§Øª."""
    date_dir = datetime.now(TZ).strftime("%Y-%m-%d")
    fb_dir   = os.path.join(OUT_DIR, "facebook", date_dir)
    img_dir  = os.path.join(OUT_DIR, "images",   date_dir)
    ensure_dir(fb_dir); ensure_dir(img_dir)

    slug = slugify(it.get("title", "post")) or "post"
    text = compose_fb_text(it)

    # Ø¬Ù„Ø¨ ØµÙˆØ± og:image
    img_urls = extract_og_images(it.get("url", "")) if it.get("url") else []
    saved_imgs = download_images(img_urls, img_dir, slug, max_n=FACEBOOK_MAX_IMAGES) if img_urls else []

    post_path = os.path.join(fb_dir, f"{slug}.txt")
    with open(post_path, "w", encoding="utf-8") as f:
        if saved_imgs:
            f.write("ğŸ“· ØµÙˆØ±/ØªÙØ§ØµÙŠÙ„ Ø£ÙƒØ«Ø± Ø¨Ø§Ù„Ø¯Ø§Ø®Ù„ â¤µï¸\n")
        f.write(text)
        f.write("\n")
    logger.info(f"Facebook post ready: {post_path} | images: {len(saved_imgs)}")
    return post_path, saved_imgs

# =============== Ø§Ù„Ø­Ù„Ù‚Ø© Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ© ===============
DEFAULT_SOURCES = [
    {"name": "Iraq News Agency (INA)", "lang": "ar", "type": "rss", "url": "https://ina.iq/rss.ashx"},
    {"name": "Alsumaria News",          "lang": "ar", "type": "rss", "url": "https://www.alsumaria.tv/rss-feed"},
    {"name": "Shafaq News",             "lang": "ar", "type": "rss", "url": "https://shafaq.com/ar/rss"},
    {"name": "Rudaw Arabic",            "lang": "ar", "type": "rss", "url": "https://www.rudawarabia.net/rss"},
    {"name": "Kurdistan24 Arabic",      "lang": "ar", "type": "rss", "url": "https://www.kurdistan24.net/ar/rss"},
    {"name": "Baghdad Today",           "lang": "ar", "type": "rss", "url": "https://baghdadtoday.news/rss"},
    {"name": "NRT Arabic",              "lang": "ar", "type": "rss", "url": "https://www.nrttv.com/ar/rss"},
]


def load_sources():
    path = os.path.join(os.getcwd(), "sources.json")
    if os.path.exists(path):
        try:
            with open(path, "r", encoding="utf-8") as f:
                arr = json.load(f)
            if isinstance(arr, list) and arr:
                logger.info(f"Loaded {len(arr)} sources from sources.json")
                return arr
        except Exception as ex:
            logger.warning(f"sources.json parsing failed: {ex}")
    return DEFAULT_SOURCES


def collect_once():
    sources = load_sources()
    total_new = 0

    row = conn.execute("SELECT val FROM meta WHERE key='zero_streak'").fetchone()
    zero_streak = int(row[0]) if row and str(row[0]).isdigit() else 0

    for src in sources:
        name = src.get("name", "?")
        if source_is_disabled(name):
            logger.warning(f"[SKIP] '{name}' Ù…Ø¹Ø·Ù‘Ù„ Ù…Ø¤Ù‚ØªÙ‹Ø§")
            continue
        try:
            items = fetch_rss(src) if src.get("type") == "rss" else fetch_scrape(src)
            logger.info(f"{name}: fetched {len(items)} items")
            if not items:
                source_mark_fail(name, cool_minutes=60)
                continue
            source_mark_ok(name)
        except Exception as ex:
            logger.error(f"Fetch failed for {name}: {ex}")
            source_mark_fail(name)
            continue

        for it in items:
            title = it.get("title") or ""
            url   = it.get("url") or ""
            content = it.get("summary") or ""
            if not title or not url:
                continue
            if is_duplicate(title, url, content):
                logger.info(f"[SKIP] duplicate/similar: {title}")
                continue
            save_item(it)

            # Telegram
            ok = send_telegram(format_message(it))
            if not ok:
                save_to_md(it)

            # Facebook
            if FACEBOOK_MODE:
                try:
                    handle_facebook(it)
                except Exception as ex:
                    logger.warning(f"facebook compose failed: {ex}")

            total_new += 1
            time.sleep(0.5)

    if total_new == 0:
        zero_streak += 1
    else:
        zero_streak = 0
    conn.execute("INSERT INTO meta(key,val) VALUES('zero_streak',?) ON CONFLICT(key) DO UPDATE SET val=?", (str(zero_streak), str(zero_streak)))
    conn.commit()

    logger.info(f"New items sent/saved: {total_new}")


def main():
    import argparse
    ap = argparse.ArgumentParser()
    ap.add_argument("--once", action="store_true", help="ØªØ´ØºÙŠÙ„ Ù…Ø±Ø© ÙˆØ§Ø­Ø¯Ø© ÙˆØ§Ù„Ø®Ø±ÙˆØ¬")
    args = ap.parse_args()
    if args.once:
        collect_once()
    else:
        while True:
            collect_once()
            time.sleep(POLL_SECONDS)


if __name__ == "__main__":
    main()


# =====================
# requirements.txt
# feedparser
# beautifulsoup4
# lxml
# python-dateutil
# requests
# =====================

# =====================
# .github/workflows/run.yml (Ù…Ø«Ø§Ù„ ØªØ´ØºÙŠÙ„ Ø¹Ù„Ù‰ GitHub Actions)
# ---------------------------------------------------------
# name: IraqNews Bot
# on:
#   workflow_dispatch:
#   schedule:
#     - cron: "*/15 * * * *"  # ÙƒÙ„ 15 Ø¯Ù‚ÙŠÙ‚Ø©
# jobs:
#   run-bot:
#     runs-on: ubuntu-latest
#     steps:
#       - name: Checkout repo
#         uses: actions/checkout@v4
#       - name: Set up Python
#         uses: actions/setup-python@v5
#         with:
#           python-version: "3.x"
#       - name: Install requirements
#         run: pip install -r requirements.txt
#       - name: Run bot (with Facebook compose)
#         env:
#           TG_TOKEN: ${{ secrets.TG_TOKEN }}
#           TG_CHAT_ID: ${{ secrets.TG_CHAT_ID }}
#           AUTO_PIP: "1"
#           FACEBOOK_MODE: "1"
#           FACEBOOK_TEMPLATE: "summary"
#           FACEBOOK_MAX_IMAGES: "3"
#         run: python news_bot.py --once
# =====================
